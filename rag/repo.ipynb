{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d1339c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fil/miniconda3/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_qdrant.fastembed_sparse import FastEmbedSparse\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "DOCS_DIR = \"docs\"  # Directory containing your pdf files\n",
    "MARKDOWN_DIR = \"markdown\" # Directory containing the pdfs converted to markdown\n",
    "PARENT_STORE_PATH = \"parent_store\"  # Directory for parent chunk JSON files\n",
    "CHILD_COLLECTION = \"document_child_chunks\"\n",
    "\n",
    "os.makedirs(DOCS_DIR, exist_ok=True)\n",
    "os.makedirs(MARKDOWN_DIR, exist_ok=True)\n",
    "os.makedirs(PARENT_STORE_PATH, exist_ok=True)\n",
    "\n",
    "# from langchain_ollama import ChatOllama\n",
    "# llm = ChatOllama(model=\"qwen3:4b-instruct-2507-q4_K_M\", temperature=0)\n",
    "\n",
    "# dense_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "# sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "client = QdrantClient(path=\"qdrant_db\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen3-8b-fp8\",\n",
    "    base_url=\"http://100.67.127.53:8000/v1\",\n",
    "    api_key=\"sk-test\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0767a95",
   "metadata": {},
   "source": [
    "## Step 2: Configure Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c4fb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1314.15it/s, Materializing param=pooler.dense.weight]                        \n",
      "\u001b[1mMPNetModel LOAD REPORT\u001b[0m from: sentence-transformers/all-mpnet-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.http.models import models as qmodels\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_qdrant.qdrant import RetrievalMode\n",
    "\n",
    "dense_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "embedding_dimension = len(dense_embeddings.embed_query(\"test\"))\n",
    "\n",
    "def ensure_collection(collection_name):\n",
    "    if not client.collection_exists(collection_name):\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=qmodels.VectorParams(\n",
    "                size=embedding_dimension,\n",
    "                distance=qmodels.Distance.COSINE\n",
    "            ),\n",
    "            sparse_vectors_config={\n",
    "                \"sparse\": qmodels.SparseVectorParams()\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd752c",
   "metadata": {},
   "source": [
    "## Step 3: PDFs to Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1076071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymupdf.layout\n",
    "import pymupdf4llm\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def pdf_to_markdown(pdf_path, output_dir):\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    md = pymupdf4llm.to_markdown(doc, headings=False, footer=False, page_separators=True, ignore_images=True, write_images=False, image_path=None)\n",
    "    md_cleaned = md.encode('utf-8', errors='surrogatepass').decode('utf-8', errors='ignore')\n",
    "    output_path = Path(output_dir) / Path(doc.name).stem\n",
    "    Path(output_path).with_suffix(\".md\").write_bytes(md_cleaned.encode('utf-8'))\n",
    "\n",
    "\n",
    "def pdfs_to_markdowns(path_pattern, overwrite: bool = False):\n",
    "    output_dir = Path(MARKDOWN_DIR)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for pdf_path in map(Path, glob.glob(path_pattern)):\n",
    "        md_path = (output_dir / pdf_path.stem).with_suffix(\".md\")\n",
    "        if overwrite or not md_path.exists():\n",
    "            pdf_to_markdown(pdf_path, output_dir)\n",
    "\n",
    "pdfs_to_markdowns(f\"{DOCS_DIR}/*.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09a7781",
   "metadata": {},
   "source": [
    "## Step 4: Hierarchical Document Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "087decef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27484d4c",
   "metadata": {},
   "source": [
    "### Processing chunk (Too small, large chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec4d53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_small_parents(chunks, min_size):\n",
    "    if not chunks:\n",
    "        return []\n",
    "\n",
    "    merged, current = [], None\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if current is None:\n",
    "            current = chunk\n",
    "        else:\n",
    "            current.page_content += \"\\n\\n\" + chunk.page_content\n",
    "            for k, v in chunk.metadata.items():\n",
    "                if k in current.metadata:\n",
    "                    current.metadata[k] = f\"{current.metadata[k]} -> {v}\"\n",
    "                else:\n",
    "                    current.metadata[k] = v\n",
    "\n",
    "        if len(current.page_content) >= min_size:\n",
    "            merged.append(current)\n",
    "            current = None\n",
    "\n",
    "    if current:\n",
    "        if merged:\n",
    "            merged[-1].page_content += \"\\n\\n\" + current.page_content\n",
    "            for k, v in current.metadata.items():\n",
    "                if k in merged[-1].metadata:\n",
    "                    merged[-1].metadata[k] = f\"{merged[-1].metadata[k]} -> {v}\"\n",
    "                else:\n",
    "                    merged[-1].metadata[k] = v\n",
    "        else:\n",
    "            merged.append(current)\n",
    "\n",
    "    return merged\n",
    "\n",
    "def split_large_parents(chunks, max_size, splitter):\n",
    "    split_chunks = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if len(chunk.page_content) <= max_size:\n",
    "            split_chunks.append(chunk)\n",
    "        else:\n",
    "            large_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=max_size,\n",
    "                chunk_overlap=splitter._chunk_overlap\n",
    "            )\n",
    "            sub_chunks = large_splitter.split_documents([chunk])\n",
    "            split_chunks.extend(sub_chunks)\n",
    "\n",
    "    return split_chunks\n",
    "\n",
    "def clean_small_chunks(chunks, min_size):\n",
    "    cleaned = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if len(chunk.page_content) < min_size:\n",
    "            if cleaned:\n",
    "                cleaned[-1].page_content += \"\\n\\n\" + chunk.page_content\n",
    "                for k, v in chunk.metadata.items():\n",
    "                    if k in cleaned[-1].metadata:\n",
    "                        cleaned[-1].metadata[k] = f\"{cleaned[-1].metadata[k]} -> {v}\"\n",
    "                    else:\n",
    "                        cleaned[-1].metadata[k] = v\n",
    "            elif i < len(chunks) - 1:\n",
    "                chunks[i + 1].page_content = chunk.page_content + \"\\n\\n\" + chunks[i + 1].page_content\n",
    "                for k, v in chunk.metadata.items():\n",
    "                    if k in chunks[i + 1].metadata:\n",
    "                        chunks[i + 1].metadata[k] = f\"{v} -> {chunks[i + 1].metadata[k]}\"\n",
    "                    else:\n",
    "                        chunks[i + 1].metadata[k] = v\n",
    "            else:\n",
    "                cleaned.append(chunk)\n",
    "        else:\n",
    "            cleaned.append(chunk)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "012e10b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if client.collection_exists(CHILD_COLLECTION):\n",
    "    client.delete_collection(CHILD_COLLECTION)\n",
    "    ensure_collection(CHILD_COLLECTION)\n",
    "else: \n",
    "    ensure_collection(CHILD_COLLECTION)\n",
    "\n",
    "child_vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=CHILD_COLLECTION,\n",
    "    embedding=dense_embeddings,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    "    sparse_vector_name=\"sparse\"\n",
    "    )\n",
    "\n",
    "def index_documents():\n",
    "    headers_to_split_on = [(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\")]\n",
    "    headers_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=100)\n",
    "\n",
    "    min_parent_size = 2000\n",
    "    max_parent_size = 4000\n",
    "\n",
    "    all_parent_pairs, all_child_chunks = [], []\n",
    "    md_files = sorted(glob.glob(os.path.join(MARKDOWN_DIR, \"*.md\")))\n",
    "\n",
    "    if not md_files:\n",
    "        return \n",
    "    \n",
    "    for doc_path_str in md_files:\n",
    "        doc_path = Path(doc_path_str)\n",
    "        try: \n",
    "            with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                md_content = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {doc_path}: {e}\")\n",
    "            continue\n",
    "        parent_chunks = headers_splitter.split_text(md_content)\n",
    "        merged_parents = merge_small_parents(parent_chunks, min_parent_size)\n",
    "        split_parents = split_large_parents(merged_parents, max_parent_size, child_splitter)\n",
    "        cleaned_parents = clean_small_chunks(split_parents, min_parent_size) \n",
    "\n",
    "        for i, p_chunks in enumerate(cleaned_parents):\n",
    "            parent_id = f\"{doc_path.steam}_parent_{i}\"\n",
    "            p_chunks.metadata.update({\"source\": doc_path.stem + \".pdf\", \"parent_id\": parent_id})\n",
    "            all_parent_pairs.append((parent_id, p_chunks))\n",
    "            children = child_splitter.split_documents([p_chunks])\n",
    "            all_child_chunks.extend(children)\n",
    "        \n",
    "    if not all_child_chunks:\n",
    "        print(f\"No child chunks created for document: {doc_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        child_vector_store.add_documents(all_child_chunks)\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding child chunks to vector store: {e}\")\n",
    "        return\n",
    "    \n",
    "    for item in os.listdir(PARENT_STORE_PATH):\n",
    "        os.remove(os.path.join(PARENT_STORE_PATH, item))\n",
    "\n",
    "    for parent_id, doc in all_parent_pairs:\n",
    "        doc_dict = {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\n",
    "        filepath = os.path.join(PARENT_STORE_PATH, f\"{parent_id}.json\")\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(doc_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "index_documents()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c42db",
   "metadata": {},
   "source": [
    "### Step 5: Define Agent Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfde833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_child_chunks(query: str, limit: str) -> str:\n",
    "    \"\"\" Search for top K relevant child chunks\n",
    "\n",
    "    Args:\n",
    "\n",
    "        query (str): The search query\n",
    "        limit (str): The maxium numbers of results to return\n",
    "    \"\"\"\n",
    "\n",
    "    try: \n",
    "        results = child_vector_store.similarity_search(query, k=int(limit), score_threshold=0.6)\n",
    "        if not results:\n",
    "            return \"No relevant chunks found.\"\n",
    "    \n",
    "        return \"\\n\\n\".join([\n",
    "            f\"Parent ID: {doc.metadata.get('parent_id', '')}\\n\"\n",
    "            f\"File Name: {doc.metadata.get('source', '')}\\n\"\n",
    "            f\"Content: {doc.page_content.strip()}\"\n",
    "            for doc in results\n",
    "        ])\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"RETRIEVAL_ERROR: {str(e)}\"\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
