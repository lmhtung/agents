{
  "page_content": "### Step 11: Create Chat Interface  \nBuild a Gradio interface with conversation persistence and human-in-the-loop support. For a complete end-to-end pipeline Gradio interface, including document ingestion, please refer to [project/README.md](./project/README.md).  \n```python\nimport gradio as gr\nimport uuid\n\ndef create_thread_id():\n\"\"\"Generate a unique thread ID for each conversation\"\"\"\nreturn {\"configurable\": {\"thread_id\": str(uuid.uuid4())}, \"recursion_limit\": 50}\n\ndef clear_session():\n\"\"\"Clear thread for new conversation\"\"\"\nglobal config\nagent_graph.checkpointer.delete_thread(config[\"configurable\"][\"thread_id\"])\nconfig = create_thread_id()\n\ndef chat_with_agent(message, history):\ncurrent_state = agent_graph.get_state(config)\n\nif current_state.next:\nagent_graph.update_state(config,{\"messages\": [HumanMessage(content=message.strip())]})\nresult = agent_graph.invoke(None, config)\nelse:\nresult = agent_graph.invoke({\"messages\": [HumanMessage(content=message.strip())]}, config)\n\nreturn result['messages'][-1].content\n\nconfig = create_thread_id()\n\nwith gr.Blocks() as demo:\nchatbot = gr.Chatbot()\nchatbot.clear(clear_session)\ngr.ChatInterface(fn=chat_with_agent, chatbot=chatbot)\n\ndemo.launch(theme=gr.themes.Citrus())\n```  \n**You're done!** You now have a fully functional Agentic RAG system with conversation memory, hierarchical indexing, and human-in-the-loop query clarification.  \n---\n\n## Modular Architecture  \nThe app (`project/` folder) is organized into modular components â€” each independently swappable without breaking the system.\n\n### ğŸ“‚ Project Structure\n```\nproject/\nâ”œâ”€â”€ app.py                    # Main Gradio application entry point\nâ”œâ”€â”€ config.py                 # Configuration hub (models, chunk sizes, providers)\nâ”œâ”€â”€ core/                     # RAG system orchestration\nâ”œâ”€â”€ db/                       # Vector DB and parent chunk storage\nâ”œâ”€â”€ rag_agent/                # LangGraph workflow (nodes, edges, prompts, tools)\nâ””â”€â”€ ui/                       # Gradio interface\n```  \nKey customization points: LLM provider, embedding model, chunking strategy, agent workflow, and system prompts â€” all configurable via `config.py` or their respective modules.  \nFull documentation in [project/README.md](./project/README.md).",
  "metadata": {
    "H2": "Implementation -> Modular Architecture -> Modular Architecture",
    "H3": "Step 11: Create Chat Interface -> ğŸ“‚ Project Structure",
    "source": "Instruct.pdf",
    "parent_id": "Instruct_parent_12"
  }
}