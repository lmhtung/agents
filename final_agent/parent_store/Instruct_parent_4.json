{
  "page_content": "### Step 4: Hierarchical Document Indexing  \nProcess documents with the Parent/Child splitting strategy.\n```python\nimport os\nimport glob\nimport json\nfrom pathlib import Path\nfrom langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n```  \n<details>\n<summary>Parent & Child chunk processing functions</summary>  \n```python\ndef merge_small_parents(chunks, min_size):\nif not chunks:\nreturn []\n\nmerged, current = [], None\n\nfor chunk in chunks:\nif current is None:\ncurrent = chunk\nelse:\ncurrent.page_content += \"\\n\\n\" + chunk.page_content\nfor k, v in chunk.metadata.items():\nif k in current.metadata:\ncurrent.metadata[k] = f\"{current.metadata[k]} -> {v}\"\nelse:\ncurrent.metadata[k] = v\n\nif len(current.page_content) >= min_size:\nmerged.append(current)\ncurrent = None\n\nif current:\nif merged:\nmerged[-1].page_content += \"\\n\\n\" + current.page_content\nfor k, v in current.metadata.items():\nif k in merged[-1].metadata:\nmerged[-1].metadata[k] = f\"{merged[-1].metadata[k]} -> {v}\"\nelse:\nmerged[-1].metadata[k] = v\nelse:\nmerged.append(current)\n\nreturn merged\n\ndef split_large_parents(chunks, max_size, splitter):\nsplit_chunks = []\n\nfor chunk in chunks:\nif len(chunk.page_content) <= max_size:\nsplit_chunks.append(chunk)\nelse:\nlarge_splitter = RecursiveCharacterTextSplitter(\nchunk_size=max_size,\nchunk_overlap=splitter._chunk_overlap\n)\nsub_chunks = large_splitter.split_documents([chunk])\nsplit_chunks.extend(sub_chunks)\n\nreturn split_chunks\n\ndef clean_small_chunks(chunks, min_size):\ncleaned = []\n\nfor i, chunk in enumerate(chunks):\nif len(chunk.page_content) < min_size:\nif cleaned:\ncleaned[-1].page_content += \"\\n\\n\" + chunk.page_content\nfor k, v in chunk.metadata.items():\nif k in cleaned[-1].metadata:\ncleaned[-1].metadata[k] = f\"{cleaned[-1].metadata[k]} -> {v}\"\nelse:\ncleaned[-1].metadata[k] = v\nelif i < len(chunks) - 1:\nchunks[i + 1].page_content = chunk.page_content + \"\\n\\n\" + chunks[i + 1].page_content\nfor k, v in chunk.metadata.items():\nif k in chunks[i + 1].metadata:\nchunks[i + 1].metadata[k] = f\"{v} -> {chunks[i + 1].metadata[k]}\"\nelse:\nchunks[i + 1].metadata[k] = v\nelse:\ncleaned.append(chunk)\nelse:\ncleaned.append(chunk)\n\nreturn cleaned\n```  \n</details>  \n```python\nif client.collection_exists(CHILD_COLLECTION):\nclient.delete_collection(CHILD_COLLECTION)\nensure_collection(CHILD_COLLECTION)\nelse:\nensure_collection(CHILD_COLLECTION)\n\nchild_vector_store = QdrantVectorStore(\nclient=client,\ncollection_name=CHILD_COLLECTION,\nembedding=dense_embeddings,\nsparse_embedding=sparse_embeddings,\nretrieval_mode=RetrievalMode.HYBRID,\nsparse_vector_name=\"sparse\"\n)\n\ndef index_documents():\nheaders_to_split_on = [(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\")]\nparent_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n\nmin_parent_size = 2000\nmax_parent_size = 4000\n\nall_parent_pairs, all_child_chunks = [], []\nmd_files = sorted(glob.glob(os.path.join(MARKDOWN_DIR, \"*.md\")))\n\nif not md_files:\nreturn\n\nfor doc_path_str in md_files:\ndoc_path = Path(doc_path_str)\ntry:\nwith open(doc_path, \"r\", encoding=\"utf-8\") as f:\nmd_text = f.read()\nexcept Exception as e:\ncontinue\n\nparent_chunks = parent_splitter.split_text(md_text)\nmerged_parents = merge_small_parents(parent_chunks, min_parent_size)\nsplit_parents = split_large_parents(merged_parents, max_parent_size, child_splitter)\ncleaned_parents = clean_small_chunks(split_parents, min_parent_size)\n\nfor i, p_chunk in enumerate(cleaned_parents):\nparent_id = f\"{doc_path.stem}_parent_{i}\"\np_chunk.metadata.update({\"source\": doc_path.stem + \".pdf\", \"parent_id\": parent_id})\nall_parent_pairs.append((parent_id, p_chunk))\nchildren = child_splitter.split_documents([p_chunk])\nall_child_chunks.extend(children)\n\nif not all_child_chunks:\nreturn\n\ntry:\nchild_vector_store.add_documents(all_child_chunks)\nexcept Exception as e:\nreturn\n\ntry:\nchild_vector_store.add_documents(all_child_chunks)\nexcept Exception as e:\nreturn\n\nfor item in os.listdir(PARENT_STORE_PATH):\nos.remove(os.path.join(PARENT_STORE_PATH, item))\n\nfor parent_id, doc in all_parent_pairs:\ndoc_dict = {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\nfilepath = os.path.join(PARENT_STORE_PATH, f\"{parent_id}.json\")\nwith open(filepath, \"w\", encoding=\"utf-8\") as f:\njson.dump(doc_dict, f, ensure_ascii=False, indent=2)\n\nindex_documents()\n```  \n---",
  "metadata": {
    "H2": "Implementation -> Implementation",
    "H3": "Step 4: Hierarchical Document Indexing -> Step 4: Hierarchical Document Indexing",
    "source": "Instruct.pdf",
    "parent_id": "Instruct_parent_4"
  }
}