{
  "page_content": "## **8. Inference**  \nĐểsửdụng hệthống trên, ta có thểdùng chếđộstream đểcác message xuất hiện lần lượt trên màn hình như sau:  \n1 `for chunk in supervisor.stream(`  \n2 `{` 3 `\"messages\" : [` 4 `{` 5 `\"role\" : \"user\" ,` 6 `\"content\" : \"What is the concept visualized in the image? Image: https :// huggingface.co/datasets/tmnam20/Storage/resolve/main/rope.png Provide me detailed information about the concept. If possible , give me some research papers about it.\" ,` 7 `}` 8 `]` 9 `},` 10 `):` 11 `pretty_print_messages (chunk , last_message=True)` 12  \n13 `final_message_history = chunk[ \"supervisor\" ][ \"messages\" ]`  \n--- end of page.page_number=17 ---  \n**AI VIETNAM**  \n**aivietnam.edu.vn**  \nKết quảđược hiển thịnhư dưới đây:  \n1  \n```\nUpdatefromnodesupervisor:\n```  \n2  \n3  \n4 `================================= Tool Message =================================` 5 `Name: transfer_to_vision_agent` 6 7 `Successfully transferred to vision_agent`  \n8  \n9  \n10 `https :// huggingface.co/datasets/tmnam20/Storage/resolve/main/rope.png` 11 `Update from node vision_agent :`  \n12 13  \n14 `================================= Tool Message =================================` 15 `Name: transfer_back_to_supervisor`  \n16  \n17 `Successfully transferred back to supervisor`  \n18  \n19  \n20 `Update from node supervisor:`  \n21  \n22  \n23 `================================= Tool Message =================================` 24 `Name: transfer_to_research_agent`  \n25  \n26 `Successfully transferred to research_agent`  \n27  \n28  \n29 `Update from node research_agent :`  \n30  \n31  \n32 `================================= Tool Message =================================` 33 `Name: transfer_back_to_supervisor`  \n34  \n35 `Successfully transferred back to supervisor`  \n36  \n37  \n38 `Update from node supervisor:`  \n39  \n40  \n41 `================================== Ai Message ==================================` 42 `Name: supervisor`  \n43  \n- 44 `The image visualizes the concept of Rotary Position Embedding (RoPE) applied in transformer models. It demonstrates how RoPE encodes positional information by rotating query and key vectors.`  \n45  \n46 `** Detailed Information **:` 47 `- ** Purpose **: RoPE enhances transformers \" ability to utilize relative positional information , crucial for understanding sequences in natural language processing and other tasks.` 48 `- ** Mechanism **: The effect is achieved by manipulating position matrices that rotate input vectors , leading to improved contextual understanding of tokens in layers of the transformer architecture.` 49 50 `** Research Papers **:` 51 `1. ** Title **: Rotary Outliers and Rotary Offset Features in Large Language Models` 52 `- ** Author **: Andre Jonasson` 53 `- ** Published **: March 3, 2025`  \n--- end of page.page_number=18 ---  \n**AI VIETNAM**  \n**aivietnam.edu.vn**  \n- 54 `- ** Summary **: The paper investigates RoPE\" s impact on attention mechanisms , identifying patterns and anomalies in queries and keys and establishing the relevance of rotary embeddings across layer and architecture differences.`  \n55  \n- 56 `You can refer to various platforms like arXiv or Google Scholar for more research -related content on Rotary Position Embedding.`  \n--- end of page.page_number=19 ---  \n**AI VIETNAM**  \n**aivietnam.edu.vn**",
  "metadata": {
    "H2": "**7. Xây dựng supervisor agent** -> **8. Inference**",
    "source": "Description-Visual-Agentic-AI.pdf",
    "parent_id": "Description-Visual-Agentic-AI_parent_10"
  }
}