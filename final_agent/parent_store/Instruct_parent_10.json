{
  "page_content": "response = llm_with_tools.invoke([sys_msg] + summary_injection + state[\"messages\"])\ntool_calls = response.tool_calls if hasattr(response, \"tool_calls\") else []\nreturn {\"messages\": [response], \"tool_call_count\": len(tool_calls) if tool_calls else 0, \"iteration_count\": 1}\n\ndef route_after_orchestrator_call(state: AgentState) -> Literal[\"tool\", \"fallback_response\", \"collect_answer\"]:\niteration = state.get(\"iteration_count\", 0)\ntool_count = state.get(\"tool_call_count\", 0)\n\nif iteration >= MAX_ITERATIONS or tool_count > MAX_TOOL_CALLS:\nreturn \"fallback_response\"\n\nlast_message = state[\"messages\"][-1]\ntool_calls = getattr(last_message, \"tool_calls\", None) or []\n\nif not tool_calls:\nreturn \"collect_answer\"\n\nreturn \"tools\"\n\ndef fallback_response(state: AgentState):\nseen = set()\nunique_contents = []\nfor m in state[\"messages\"]:\nif isinstance(m, ToolMessage) and m.content not in seen:\nunique_contents.append(m.content)\nseen.add(m.content)\n\ncontext_summary = state.get(\"context_summary\", \"\").strip()\n\ncontext_parts = []\nif context_summary:\ncontext_parts.append(f\"## Compressed Research Context (from prior iterations)\\n\\n{context_summary}\")\nif unique_contents:\ncontext_parts.append(\n\"## Retrieved Data (current iteration)\\n\\n\" +\n\"\\n\\n\".join(f\"--- DATA SOURCE {i} ---\\n{content}\" for i, content in enumerate(unique_contents, 1))\n)\n\ncontext_text = \"\\n\\n\".join(context_parts) if context_parts else \"No data was retrieved from the documents.\"\n\nprompt_content = (\nf\"USER QUERY: {state.get('question')}\\n\\n\"\nf\"{context_text}\\n\\n\"\nf\"INSTRUCTION:\\nProvide the best possible answer using only the data above.\"\n)\nresponse = llm.invoke([SystemMessage(content=get_fallback_response_prompt()), HumanMessage(content=prompt_content)])\nreturn {\"messages\": [response]}\n\ndef should_compress_context(state: AgentState) -> Command[Literal[\"compress_context\", \"orchestrator\"]]:\nmessages = state[\"messages\"]\n\nnew_ids: Set[str] = set()\nfor msg in reversed(messages):\nif isinstance(msg, AIMessage) and getattr(msg, \"tool_calls\", None):\nfor tc in msg.tool_calls:\nif tc[\"name\"] == \"retrieve_parent_chunks\":\nraw = tc[\"args\"].get(\"parent_id\") or tc[\"args\"].get(\"id\") or tc[\"args\"].get(\"ids\") or []\nif isinstance(raw, str):\nnew_ids.add(f\"parent::{raw}\")\nelse:\nnew_ids.update(f\"parent::{r}\" for r in raw)\n\nelif tc[\"name\"] == \"search_child_chunks\":\nquery = tc[\"args\"].get(\"query\", \"\")\nif query:\nnew_ids.add(f\"search::{query}\")\nbreak\n\nupdated_ids = state.get(\"retrieval_keys\", set()) | new_ids\n\ncurrent_token_messages = estimate_context_tokens(messages)\ncurrent_token_summary = estimate_context_tokens([HumanMessage(content=state.get(\"context_summary\", \"\"))])\ncurrent_tokens = current_token_messages + current_token_summary\n\nmax_allowed = BASE_TOKEN_THRESHOLD + int(current_token_summary * TOKEN_GROWTH_FACTOR)\n\ngoto = \"compress_context\" if current_tokens > max_allowed else \"orchestrator\"\nreturn Command(update={\"retrieval_keys\": updated_ids}, goto=goto)\n\ndef compress_context(state: AgentState):\nmessages = state[\"messages\"]\nexisting_summary = state.get(\"context_summary\", \"\").strip()\n\nif not messages:\nreturn {}\n\nconversation_text = f\"USER QUESTION:\\n{state.get('question')}\\n\\nConversation to compress:\\n\\n\"\nif existing_summary:\nconversation_text += f\"[PRIOR COMPRESSED CONTEXT]\\n{existing_summary}\\n\\n\"\n\nfor msg in messages[1:]:\nif isinstance(msg, AIMessage):\ntool_calls_info = \"\"\nif getattr(msg, \"tool_calls\", None):\ncalls = \", \".join(f\"{tc['name']}({tc['args']})\" for tc in msg.tool_calls)\ntool_calls_info = f\" | Tool calls: {calls}\"\nconversation_text += f\"[ASSISTANT{tool_calls_info}]\\n{msg.content or '(tool call only)'}\\n\\n\"\nelif isinstance(msg, ToolMessage):\ntool_name = getattr(msg, \"name\", \"tool\")\nconversation_text += f\"[TOOL RESULT — {tool_name}]\\n{msg.content}\\n\\n\"\n\nsummary_response = llm.invoke([SystemMessage(content=get_context_compression_prompt()), HumanMessage(content=conversation_text)])\nnew_summary = summary_response.content\n\nretrieved_ids: Set[str] = state.get(\"retrieval_keys\", set())\nif retrieved_ids:\nparent_ids = sorted(r for r in retrieved_ids if r.startswith(\"parent::\"))\nsearch_queries = sorted(r.replace(\"search::\", \"\") for r in retrieved_ids if r.startswith(\"search::\"))\n\nblock = \"\\n\\n---\\n**Already executed (do NOT repeat):**\\n\"\nif parent_ids:\nblock += \"Parent chunks retrieved:\\n\" + \"\\n\".join(f\"- {p.replace('parent::', '')}\" for p in parent_ids) + \"\\n\"\nif search_queries:\nblock += \"Search queries already run:\\n\" + \"\\n\".join(f\"- {q}\" for q in search_queries) + \"\\n\"\nnew_summary += block\n\nreturn {\"context_summary\": new_summary, \"messages\": [RemoveMessage(id=m.id) for m in messages[1:]]}\n\ndef collect_answer(state: AgentState):\nlast_message = state[\"messages\"][-1]\nis_valid = isinstance(last_message, AIMessage) and last_message.content and not last_message.tool_calls\nanswer = last_message.content if is_valid else \"Unable to generate an answer.\"\nreturn {\n\"final_answer\": answer,\n\"agent_answers\": [{\"index\": state[\"question_index\"], \"question\": state[\"question\"], \"answer\": answer}]\n}\n```  \n**Why this architecture?**\n- **Summarization** maintains conversational context without overwhelming the LLM\n- **Query rewriting** ensures search queries are precise and unambiguous, using context intelligently\n- **Human-in-the-loop** catches unclear queries before wasting any retrieval resources\n- **Parallel execution** via `Send` API spawns independent agent subgraphs for each sub-question simultaneously\n- **Context compression** keeps the agent's working memory lean across long retrieval loops, preventing redundant fetches\n- **Fallback response** ensures graceful degradation — the agent always returns something useful even when the budget runs out\n- **Answer collection & aggregation** extracts clean final answers from agents and aggregates them into a single coherent response\n---",
  "metadata": {
    "H2": "Implementation -> Implementation",
    "H3": "Step 9: Build Graph Node and Edge Functions -> Step 9: Build Graph Node and Edge Functions",
    "source": "Instruct.pdf",
    "parent_id": "Instruct_parent_10"
  }
}