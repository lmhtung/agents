{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d1339c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fil/miniconda3/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_qdrant.fastembed_sparse import FastEmbedSparse\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "DOCS_DIR = \"docs\"  # Directory containing your pdf files\n",
    "MARKDOWN_DIR = \"markdown\" # Directory containing the pdfs converted to markdown\n",
    "PARENT_STORE_PATH = \"parent_store\"  # Directory for parent chunk JSON files\n",
    "CHILD_COLLECTION = \"document_child_chunks\"\n",
    "\n",
    "os.makedirs(DOCS_DIR, exist_ok=True)\n",
    "os.makedirs(MARKDOWN_DIR, exist_ok=True)\n",
    "os.makedirs(PARENT_STORE_PATH, exist_ok=True)\n",
    "\n",
    "# from langchain_ollama import ChatOllama\n",
    "# llm = ChatOllama(model=\"qwen3:4b-instruct-2507-q4_K_M\", temperature=0)\n",
    "\n",
    "# dense_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "# sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "client = QdrantClient(path=\"qdrant_db\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen3-8b-fp8\",\n",
    "    base_url=\"http://100.67.127.53:8000/v1\",\n",
    "    api_key=\"sk-test\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0767a95",
   "metadata": {},
   "source": [
    "## Step 2: Configure Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c4fb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 1982.04it/s, Materializing param=pooler.dense.weight]                        \n",
      "\u001b[1mMPNetModel LOAD REPORT\u001b[0m from: sentence-transformers/all-mpnet-base-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "/home/fil/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.http.models import models as qmodels\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_qdrant.qdrant import RetrievalMode\n",
    "\n",
    "dense_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "embedding_dimension = len(dense_embeddings.embed_query(\"test\"))\n",
    "\n",
    "def ensure_collection(collection_name):\n",
    "    if not client.collection_exists(collection_name):\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=qmodels.VectorParams(\n",
    "                size=embedding_dimension,\n",
    "                distance=qmodels.Distance.COSINE\n",
    "            ),\n",
    "            sparse_vectors_config={\n",
    "                \"sparse\": qmodels.SparseVectorParams()\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd752c",
   "metadata": {},
   "source": [
    "## Step 3: PDFs to Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1076071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymupdf.layout\n",
    "import pymupdf4llm\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def pdf_to_markdown(pdf_path, output_dir):\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    md = pymupdf4llm.to_markdown(doc, headings=False, footer=False, page_separators=True, ignore_images=True, write_images=False, image_path=None)\n",
    "    md_cleaned = md.encode('utf-8', errors='surrogatepass').decode('utf-8', errors='ignore')\n",
    "    output_path = Path(output_dir) / Path(doc.name).stem\n",
    "    Path(output_path).with_suffix(\".md\").write_bytes(md_cleaned.encode('utf-8'))\n",
    "\n",
    "\n",
    "def pdfs_to_markdowns(path_pattern, overwrite: bool = False):\n",
    "    output_dir = Path(MARKDOWN_DIR)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for pdf_path in map(Path, glob.glob(path_pattern)):\n",
    "        md_path = (output_dir / pdf_path.stem).with_suffix(\".md\")\n",
    "        if overwrite or not md_path.exists():\n",
    "            pdf_to_markdown(pdf_path, output_dir)\n",
    "\n",
    "pdfs_to_markdowns(f\"{DOCS_DIR}/*.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09a7781",
   "metadata": {},
   "source": [
    "## Step 4: Hierarchical Document Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "087decef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27484d4c",
   "metadata": {},
   "source": [
    "### Processing chunk (Too small, large chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec4d53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_small_parents(chunks, min_size):\n",
    "    if not chunks:\n",
    "        return []\n",
    "\n",
    "    merged, current = [], None\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if current is None:\n",
    "            current = chunk\n",
    "        else:\n",
    "            current.page_content += \"\\n\\n\" + chunk.page_content\n",
    "            for k, v in chunk.metadata.items():\n",
    "                if k in current.metadata:\n",
    "                    current.metadata[k] = f\"{current.metadata[k]} -> {v}\"\n",
    "                else:\n",
    "                    current.metadata[k] = v\n",
    "\n",
    "        if len(current.page_content) >= min_size:\n",
    "            merged.append(current)\n",
    "            current = None\n",
    "\n",
    "    if current:\n",
    "        if merged:\n",
    "            merged[-1].page_content += \"\\n\\n\" + current.page_content\n",
    "            for k, v in current.metadata.items():\n",
    "                if k in merged[-1].metadata:\n",
    "                    merged[-1].metadata[k] = f\"{merged[-1].metadata[k]} -> {v}\"\n",
    "                else:\n",
    "                    merged[-1].metadata[k] = v\n",
    "        else:\n",
    "            merged.append(current)\n",
    "\n",
    "    return merged\n",
    "\n",
    "def split_large_parents(chunks, max_size, splitter):\n",
    "    split_chunks = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if len(chunk.page_content) <= max_size:\n",
    "            split_chunks.append(chunk)\n",
    "        else:\n",
    "            large_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=max_size,\n",
    "                chunk_overlap=splitter._chunk_overlap\n",
    "            )\n",
    "            sub_chunks = large_splitter.split_documents([chunk])\n",
    "            split_chunks.extend(sub_chunks)\n",
    "\n",
    "    return split_chunks\n",
    "\n",
    "def clean_small_chunks(chunks, min_size):\n",
    "    cleaned = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if len(chunk.page_content) < min_size:\n",
    "            if cleaned:\n",
    "                cleaned[-1].page_content += \"\\n\\n\" + chunk.page_content\n",
    "                for k, v in chunk.metadata.items():\n",
    "                    if k in cleaned[-1].metadata:\n",
    "                        cleaned[-1].metadata[k] = f\"{cleaned[-1].metadata[k]} -> {v}\"\n",
    "                    else:\n",
    "                        cleaned[-1].metadata[k] = v\n",
    "            elif i < len(chunks) - 1:\n",
    "                chunks[i + 1].page_content = chunk.page_content + \"\\n\\n\" + chunks[i + 1].page_content\n",
    "                for k, v in chunk.metadata.items():\n",
    "                    if k in chunks[i + 1].metadata:\n",
    "                        chunks[i + 1].metadata[k] = f\"{v} -> {chunks[i + 1].metadata[k]}\"\n",
    "                    else:\n",
    "                        chunks[i + 1].metadata[k] = v\n",
    "            else:\n",
    "                cleaned.append(chunk)\n",
    "        else:\n",
    "            cleaned.append(chunk)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012e10b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if client.collection_exists(CHILD_COLLECTION):\n",
    "    client.delete_collection(CHILD_COLLECTION)\n",
    "    ensure_collection(CHILD_COLLECTION)\n",
    "else: \n",
    "    ensure_collection(CHILD_COLLECTION)\n",
    "\n",
    "child_vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=CHILD_COLLECTION,\n",
    "    embedding=dense_embeddings,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    "    sparse_vector_name=\"sparse\"\n",
    "    )\n",
    "\n",
    "def index_documents():\n",
    "    headers_to_split_on = [(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\")]\n",
    "    headers_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=100)\n",
    "\n",
    "    min_parent_size = 2000\n",
    "    max_parent_size = 4000\n",
    "\n",
    "    all_parent_pairs, all_child_chunks = [], []\n",
    "    md_files = sorted(glob.glob(os.path.join(MARKDOWN_DIR, \"*.md\")))\n",
    "\n",
    "    if not md_files:\n",
    "        return \n",
    "    \n",
    "    for doc_path_str in md_files:\n",
    "        doc_path = Path(doc_path_str)\n",
    "        try: \n",
    "            with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                md_content = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {doc_path}: {e}\")\n",
    "            continue\n",
    "        parent_chunks = headers_splitter.split_text(md_content)\n",
    "        merged_parents = merge_small_parents(parent_chunks, min_parent_size)\n",
    "        split_parents = split_large_parents(merged_parents, max_parent_size, child_splitter)\n",
    "        cleaned_parents = clean_small_chunks(split_parents, min_parent_size) \n",
    "\n",
    "        for i, p_chunks in enumerate(cleaned_parents):\n",
    "            parent_id = f\"{doc_path.stem}_parent_{i}\"\n",
    "            p_chunks.metadata.update({\"source\": doc_path.stem + \".pdf\", \"parent_id\": parent_id})\n",
    "            all_parent_pairs.append((parent_id, p_chunks))\n",
    "            children = child_splitter.split_documents([p_chunks])\n",
    "            all_child_chunks.extend(children)\n",
    "        \n",
    "    if not all_child_chunks:\n",
    "        print(f\"No child chunks created for document: {doc_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        child_vector_store.add_documents(all_child_chunks)\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding child chunks to vector store: {e}\")\n",
    "        return\n",
    "    \n",
    "    for item in os.listdir(PARENT_STORE_PATH):\n",
    "        os.remove(os.path.join(PARENT_STORE_PATH, item))\n",
    "\n",
    "    for parent_id, doc in all_parent_pairs:\n",
    "        doc_dict = {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\n",
    "        filepath = os.path.join(PARENT_STORE_PATH, f\"{parent_id}.json\")\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(doc_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "index_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c42db",
   "metadata": {},
   "source": [
    "### Step 5: Define Agent Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dfde833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_child_chunks(query: str, limit: str) -> str:\n",
    "    \"\"\" Search for top K relevant child chunks\n",
    "\n",
    "    Args:\n",
    "\n",
    "        query (str): The search query\n",
    "        limit (str): The maxium numbers of results to return\n",
    "    \"\"\"\n",
    "\n",
    "    try: \n",
    "        results = child_vector_store.similarity_search(query, k=int(limit), score_threshold=0.6)\n",
    "        if not results:\n",
    "            return \"No relevant chunks found.\"\n",
    "    \n",
    "        return \"\\n\\n\".join([\n",
    "            f\"Parent ID: {doc.metadata.get('parent_id', '')}\\n\"\n",
    "            f\"File Name: {doc.metadata.get('source', '')}\\n\"\n",
    "            f\"Content: {doc.page_content.strip()}\"\n",
    "            for doc in results\n",
    "        ])\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"RETRIEVAL_ERROR: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def retrieve_parent_chunks(parent_id: str) -> str:\n",
    "    \"\"\" Retrieve parent chunk content by parent ID\n",
    "\n",
    "    Args:\n",
    "        parent_id (str): The ID of the parent chunk to retrieve\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = parent_id if parent_id.lower().endswith(\".json\") else f\"{parent_id}.json\"\n",
    "    path = os.path.join(PARENT_STORE_PATH, file_name)\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        return \"NO_PARENT_DOCUMENT\"\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    return (\n",
    "        f\"Parent ID: {parent_id}\\n\"\n",
    "        f\"File Name: {data.get('metadata', {}).get('source', 'unknown')}\\n\"\n",
    "        f\"Content: {data.get('page_content', '').strip()}\"\n",
    "    )\n",
    "\n",
    "llm_with_tools = llm.bind_tools([search_child_chunks, retrieve_parent_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb769088",
   "metadata": {},
   "source": [
    "### Step 6: Design System Prompts\n",
    "Update final_agent_with_rag/rag_agent/prompts.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007be6aa",
   "metadata": {},
   "source": [
    "### Step 7: Define State and Data Models\n",
    "Create the state structure for conversation tracking and agent execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f65aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Annotated, Set\n",
    "import operator\n",
    "\n",
    "def accumulate_or_reset(existing: List[dict], new: List[dict]) -> List[dict]:\n",
    "    if new and any(item.get('__reset__') for item in new):\n",
    "        return []\n",
    "    return existing + new\n",
    "\n",
    "def set_union(a: Set[str], b: Set[str]) -> Set[str]:\n",
    "    return a | b\n",
    "\n",
    "class State(MessagesState):\n",
    "    questionIsClear: bool = False\n",
    "    conversation_summary: str = \"\"\n",
    "    originalQuery: str = \"\"\n",
    "    rewrittenQuestions: List[str] = []\n",
    "    agent_answers: Annotated[List[dict], accumulate_or_reset] = []\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    tool_call_count: Annotated[int, operator.add] = 0\n",
    "    iteration_count: Annotated[int, operator.add] = 0\n",
    "    question: str = \"\"\n",
    "    question_index: int = 0\n",
    "    context_summary: str = \"\"\n",
    "    retrieval_keys: Annotated[Set[str], set_union] = set()\n",
    "    final_answer: str = \"\"\n",
    "    agent_answers: List[dict] = []\n",
    "\n",
    "class QueryAnalysis(BaseModel):\n",
    "    is_clear: bool = Field(description=\"Indicates if the user's question is clear and answerable.\")\n",
    "    questions: List[str] = Field(description=\"List of rewritten, self-contained questions.\")\n",
    "    clarification_needed: str = Field(description=\"Explanation if the question is unclear.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d628b59",
   "metadata": {},
   "source": [
    "### Step 8: Agent Configuration\n",
    "Hard limits on tool calls and iterations prevent infinite loops. Token counting (via tiktoken) drives context compression decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "902ceef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "MAX_TOOL_CALLS = 8       # Maximum tool calls per agent run\n",
    "MAX_ITERATIONS = 10      # Maximum agent loop iterations\n",
    "BASE_TOKEN_THRESHOLD = 2000     # Initial token threshold for compression\n",
    "TOKEN_GROWTH_FACTOR = 0.9       # Multiplier applied after each compression\n",
    "\n",
    "def estimate_context_tokens(messages: list) -> int:\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    except:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return sum(len(encoding.encode(str(msg.content))) for msg in messages if hasattr(msg, 'content') and msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39636206",
   "metadata": {},
   "source": [
    "### Step 9: Build Graph Node and Edge Functions\n",
    "Create the processing nodes and edges for the LangGraph workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2579ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main graph nodes and edges\n",
    "\n",
    "from langgraph.types import Send, Command \n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, RemoveMessage, ToolMessage\n",
    "from typing import Literal\n",
    "from prompts import get_conversation_summary_prompt, get_rewrite_query_prompt, get_aggregation_prompt\n",
    "\n",
    "\n",
    "# Tóm tắt lại nội dung cuộc hội thoại trước đó để làm ngắn bớt context nếu nó quá dài, \n",
    "# chỉ tóm tắt những phần có chứa thông tin hữu ích (các câu hỏi và câu trả lời của agent, bỏ qua các tool calls)\n",
    "def summarize_history(state: State):\n",
    "    if len(state[\"messages\"]) <= 4:\n",
    "        return {\"conversation_summary\": \"\"}\n",
    "    relevant_msgs = [\n",
    "        msg for msg in state[\"messages\"][:-1]\n",
    "        if isinstance(msg, (HumanMessage, AIMessage)) and not getattr(msg, \"tool_calls\", None)\n",
    "    ]\n",
    "\n",
    "    if not relevant_msgs:\n",
    "        return {\"conversation_summary\": \"\"}\n",
    "\n",
    "    conversation = \"Conversation history:\\n\"\n",
    "    for msg in relevant_msgs[-6:]:\n",
    "        role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
    "        conversation += f\"{role}: {msg.content}\\n\"\n",
    "\n",
    "    summary_response = llm.with_config(temperature=0.2).invoke([SystemMessage(content= get_conversation_summary_prompt()), HumanMessage(content=conversation)])\n",
    "    return {\"conversation_summary\": summary_response.content, \"agent_answers\": [{\"__reset__\": True}]}\n",
    "\n",
    "def rewrite_query(state: State):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    conversation_summary = state.get(\"conversation_summary\", \"\")\n",
    "\n",
    "    context_section = (f\"Conversation Context:\\n{conversation_summary}\\n\" if conversation_summary.strip() else \"\") + f\"User Query:\\n{last_message.content}\\n\"\n",
    "\n",
    "    llm_with_structure = llm.with_config(temperature=0.1).with_structured_output(QueryAnalysis)\n",
    "    response = llm_with_structure.invoke([SystemMessage(content=get_rewrite_query_prompt()), HumanMessage(content=context_section)])\n",
    "\n",
    "    if response.questions and response.is_clear:\n",
    "        delete_all = [RemoveMessage(id=m.id) for m in state[\"messages\"] if not isinstance(m, SystemMessage)]\n",
    "        return {\"questionIsClear\": True, \"messages\": delete_all, \"originalQuery\": last_message.content, \"rewrittenQuestions\": response.questions}\n",
    "\n",
    "    clarification = response.clarification_needed if response.clarification_needed and len(response.clarification_needed.strip()) > 10 else \"I need more information to understand your question.\"\n",
    "    return {\"questionIsClear\": False, \"messages\": [AIMessage(content=clarification)]}\n",
    "\n",
    "def request_clarification(state: State):\n",
    "    return {}\n",
    "\n",
    "def route_after_rewrite(state: State) -> Literal[\"request_clarification\", \"agent\"]:\n",
    "    if not state.get(\"questionIsClear\", False):\n",
    "        return \"request_clarification\"\n",
    "    else:\n",
    "        return [\n",
    "                Send(\"agent\", {\"question\": query, \"question_index\": idx, \"messages\": []})\n",
    "                for idx, query in enumerate(state[\"rewrittenQuestions\"])\n",
    "            ]\n",
    "\n",
    "def aggregate_answers(state: State):\n",
    "    if not state.get(\"agent_answers\"):\n",
    "        return {\"messages\": [AIMessage(content=\"No answers were generated.\")]}\n",
    "\n",
    "    sorted_answers = sorted(state[\"agent_answers\"], key=lambda x: x[\"index\"])\n",
    "\n",
    "    formatted_answers = \"\"\n",
    "    for i, ans in enumerate(sorted_answers, start=1):\n",
    "        formatted_answers += (f\"\\nAnswer {i}:\\n\"f\"{ans['answer']}\\n\")\n",
    "\n",
    "    user_message = HumanMessage(content=f\"\"\"Original user question: {state[\"originalQuery\"]}\\nRetrieved answers:{formatted_answers}\"\"\")\n",
    "    synthesis_response = llm.invoke([SystemMessage(content=get_aggregation_prompt()), user_message])\n",
    "    return {\"messages\": [AIMessage(content=synthesis_response.content)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed0c54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: AgentState):\n",
    "    context_summary = state.get(\"context_summary\", \"\").strip()\n",
    "    sys_msg = SystemMessage(content=get_orchestrator_prompt())\n",
    "    summary_injection = (\n",
    "        [HumanMessage(content=f\"[COMPRESSED CONTEXT FROM PRIOR RESEARCH]\\n\\n{context_summary}\")]\n",
    "        if context_summary else []\n",
    "    )\n",
    "    if not state.get(\"messages\"):\n",
    "        human_msg = HumanMessage(content=state[\"question\"])\n",
    "        force_search = HumanMessage(content=\"YOU MUST CALL 'search_child_chunks' AS THE FIRST STEP TO ANSWER THIS QUESTION.\")\n",
    "        response = llm_with_tools.invoke([sys_msg] + summary_injection + [human_msg, force_search])\n",
    "        return {\"messages\": [human_msg, response], \"tool_call_count\": len(response.tool_calls or []), \"iteration_count\": 1}\n",
    "\n",
    "    response = llm_with_tools.invoke([sys_msg] + summary_injection + state[\"messages\"])\n",
    "    tool_calls = response.tool_calls if hasattr(response, \"tool_calls\") else []\n",
    "    return {\"messages\": [response], \"tool_call_count\": len(tool_calls) if tool_calls else 0, \"iteration_count\": 1}\n",
    "\n",
    "def route_after_orchestrator_call(state: AgentState) -> Literal[\"tool\", \"fallback_response\", \"collect_answer\"]:\n",
    "    iteration = state.get(\"iteration_count\", 0)\n",
    "    tool_count = state.get(\"tool_call_count\", 0)\n",
    "\n",
    "    if iteration >= MAX_ITERATIONS or tool_count > MAX_TOOL_CALLS:\n",
    "        return \"fallback_response\"\n",
    "\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    tool_calls = getattr(last_message, \"tool_calls\", None) or []\n",
    "\n",
    "    if not tool_calls:\n",
    "        return \"collect_answer\"\n",
    "    \n",
    "    return \"tools\"\n",
    "\n",
    "def fallback_response(state: AgentState):\n",
    "    seen = set()\n",
    "    unique_contents = []\n",
    "    for m in state[\"messages\"]:\n",
    "        if isinstance(m, ToolMessage) and m.content not in seen:\n",
    "            unique_contents.append(m.content)\n",
    "            seen.add(m.content)\n",
    "\n",
    "    context_summary = state.get(\"context_summary\", \"\").strip()\n",
    "\n",
    "    context_parts = []\n",
    "    if context_summary:\n",
    "        context_parts.append(f\"## Compressed Research Context (from prior iterations)\\n\\n{context_summary}\")\n",
    "    if unique_contents:\n",
    "        context_parts.append(\n",
    "            \"## Retrieved Data (current iteration)\\n\\n\" +\n",
    "            \"\\n\\n\".join(f\"--- DATA SOURCE {i} ---\\n{content}\" for i, content in enumerate(unique_contents, 1))\n",
    "        )\n",
    "\n",
    "    context_text = \"\\n\\n\".join(context_parts) if context_parts else \"No data was retrieved from the documents.\"\n",
    "\n",
    "    prompt_content = (\n",
    "        f\"USER QUERY: {state.get('question')}\\n\\n\"\n",
    "        f\"{context_text}\\n\\n\"\n",
    "        f\"INSTRUCTION:\\nProvide the best possible answer using only the data above.\"\n",
    "    )\n",
    "    response = llm.invoke([SystemMessage(content=get_fallback_response_prompt()), HumanMessage(content=prompt_content)])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_compress_context(state: AgentState) -> Command[Literal[\"compress_context\", \"orchestrator\"]]:\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    new_ids: Set[str] = set()\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, AIMessage) and getattr(msg, \"tool_calls\", None):\n",
    "            for tc in msg.tool_calls:\n",
    "                if tc[\"name\"] == \"retrieve_parent_chunks\":\n",
    "                    raw = tc[\"args\"].get(\"parent_id\") or tc[\"args\"].get(\"id\") or tc[\"args\"].get(\"ids\") or []\n",
    "                    if isinstance(raw, str):\n",
    "                        new_ids.add(f\"parent::{raw}\")\n",
    "                    else:\n",
    "                        new_ids.update(f\"parent::{r}\" for r in raw)\n",
    "\n",
    "                elif tc[\"name\"] == \"search_child_chunks\":\n",
    "                    query = tc[\"args\"].get(\"query\", \"\")\n",
    "                    if query:\n",
    "                        new_ids.add(f\"search::{query}\")\n",
    "            break\n",
    "\n",
    "    updated_ids = state.get(\"retrieval_keys\", set()) | new_ids\n",
    "\n",
    "    current_token_messages = estimate_context_tokens(messages)\n",
    "    current_token_summary = estimate_context_tokens([HumanMessage(content=state.get(\"context_summary\", \"\"))])\n",
    "    current_tokens = current_token_messages + current_token_summary\n",
    "\n",
    "    max_allowed = BASE_TOKEN_THRESHOLD + int(current_token_summary * TOKEN_GROWTH_FACTOR)\n",
    "\n",
    "    goto = \"compress_context\" if current_tokens > max_allowed else \"orchestrator\"\n",
    "    return Command(update={\"retrieval_keys\": updated_ids}, goto=goto)\n",
    "\n",
    "def compress_context(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    existing_summary = state.get(\"context_summary\", \"\").strip()\n",
    "\n",
    "    if not messages:\n",
    "        return {}\n",
    "\n",
    "    conversation_text = f\"USER QUESTION:\\n{state.get('question')}\\n\\nConversation to compress:\\n\\n\"\n",
    "    if existing_summary:\n",
    "        conversation_text += f\"[PRIOR COMPRESSED CONTEXT]\\n{existing_summary}\\n\\n\"\n",
    "\n",
    "    for msg in messages[1:]:\n",
    "        if isinstance(msg, AIMessage):\n",
    "            tool_calls_info = \"\"\n",
    "            if getattr(msg, \"tool_calls\", None):\n",
    "                calls = \", \".join(f\"{tc['name']}({tc['args']})\" for tc in msg.tool_calls)\n",
    "                tool_calls_info = f\" | Tool calls: {calls}\"\n",
    "            conversation_text += f\"[ASSISTANT{tool_calls_info}]\\n{msg.content or '(tool call only)'}\\n\\n\"\n",
    "        elif isinstance(msg, ToolMessage):\n",
    "            tool_name = getattr(msg, \"name\", \"tool\")\n",
    "            conversation_text += f\"[TOOL RESULT — {tool_name}]\\n{msg.content}\\n\\n\"\n",
    "\n",
    "    summary_response = llm.invoke([SystemMessage(content=get_context_compression_prompt()), HumanMessage(content=conversation_text)])\n",
    "    new_summary = summary_response.content\n",
    "\n",
    "    retrieved_ids: Set[str] = state.get(\"retrieval_keys\", set())\n",
    "    if retrieved_ids:\n",
    "        parent_ids = sorted(r for r in retrieved_ids if r.startswith(\"parent::\"))\n",
    "        search_queries = sorted(r.replace(\"search::\", \"\") for r in retrieved_ids if r.startswith(\"search::\"))\n",
    "\n",
    "        block = \"\\n\\n---\\n**Already executed (do NOT repeat):**\\n\"\n",
    "        if parent_ids:\n",
    "            block += \"Parent chunks retrieved:\\n\" + \"\\n\".join(f\"- {p.replace('parent::', '')}\" for p in parent_ids) + \"\\n\"\n",
    "        if search_queries:\n",
    "            block += \"Search queries already run:\\n\" + \"\\n\".join(f\"- {q}\" for q in search_queries) + \"\\n\"\n",
    "        new_summary += block\n",
    "\n",
    "    return {\"context_summary\": new_summary, \"messages\": [RemoveMessage(id=m.id) for m in messages[1:]]}\n",
    "\n",
    "def collect_answer(state: AgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    is_valid = isinstance(last_message, AIMessage) and last_message.content and not last_message.tool_calls\n",
    "    answer = last_message.content if is_valid else \"Unable to generate an answer.\"\n",
    "    return {\n",
    "        \"final_answer\": answer,\n",
    "        \"agent_answers\": [{\"index\": state[\"question_index\"], \"question\": state[\"question\"], \"answer\": answer}]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d5774",
   "metadata": {},
   "source": [
    "### Step 10: Build the LangGraph Graphs\n",
    "\n",
    "Assemble the complete workflow graph with conversation memory and multi-agent architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "841c5927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "agent_builder = StateGraph(AgentState)\n",
    "agent_builder.add_node(orchestrator)\n",
    "agent_builder.add_node(\"tools\", ToolNode([search_child_chunks, retrieve_parent_chunks]))\n",
    "agent_builder.add_node(compress_context)\n",
    "agent_builder.add_node(fallback_response)\n",
    "agent_builder.add_node(should_compress_context)\n",
    "agent_builder.add_node(collect_answer)\n",
    "\n",
    "agent_builder.add_edge(START, \"orchestrator\")\n",
    "agent_builder.add_conditional_edges(\"orchestrator\", route_after_orchestrator_call, {\"tools\": \"tools\", \"fallback_response\": \"fallback_response\", \"collect_answer\": \"collect_answer\"})\n",
    "agent_builder.add_edge(\"tools\", \"should_compress_context\")\n",
    "agent_builder.add_edge(\"compress_context\", \"orchestrator\")\n",
    "agent_builder.add_edge(\"fallback_response\", \"collect_answer\")\n",
    "agent_builder.add_edge(\"collect_answer\", END)\n",
    "agent_subgraph = agent_builder.compile()\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(summarize_history)\n",
    "graph_builder.add_node(rewrite_query)\n",
    "graph_builder.add_node(request_clarification)\n",
    "graph_builder.add_node(\"agent\", agent_subgraph)\n",
    "graph_builder.add_node(aggregate_answers)\n",
    "\n",
    "graph_builder.add_edge(START, \"summarize_history\")\n",
    "graph_builder.add_edge(\"summarize_history\", \"rewrite_query\")\n",
    "graph_builder.add_conditional_edges(\"rewrite_query\", route_after_rewrite)\n",
    "graph_builder.add_edge(\"request_clarification\", \"rewrite_query\")\n",
    "graph_builder.add_edge([\"agent\"], \"aggregate_answers\")\n",
    "graph_builder.add_edge(\"aggregate_answers\", END)\n",
    "\n",
    "agent_graph = graph_builder.compile(checkpointer=checkpointer, interrupt_before=[\"request_clarification\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4615a117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import uuid\n",
    "\n",
    "def create_thread_id():\n",
    "    \"\"\"Generate a unique thread ID for each conversation\"\"\"\n",
    "    return {\"configurable\": {\"thread_id\": str(uuid.uuid4())}, \"recursion_limit\": 50}\n",
    "\n",
    "def clear_session():\n",
    "    \"\"\"Clear thread for new conversation\"\"\"\n",
    "    global config\n",
    "    agent_graph.checkpointer.delete_thread(config[\"configurable\"][\"thread_id\"])\n",
    "    config = create_thread_id()\n",
    "\n",
    "def chat_with_agent(message, history):\n",
    "    current_state = agent_graph.get_state(config)\n",
    "    \n",
    "    if current_state.next:\n",
    "        agent_graph.update_state(config,{\"messages\": [HumanMessage(content=message.strip())]})\n",
    "        result = agent_graph.invoke(None, config)\n",
    "    else:\n",
    "        result = agent_graph.invoke({\"messages\": [HumanMessage(content=message.strip())]}, config)\n",
    "    \n",
    "    return result['messages'][-1].content\n",
    "\n",
    "config = create_thread_id()\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    chatbot.clear(clear_session)\n",
    "    gr.ChatInterface(fn=chat_with_agent, chatbot=chatbot)\n",
    "\n",
    "demo.launch(theme=gr.themes.Citrus())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
